# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y8lbpctEkuAin1aqbL5ONrCROZEN287a
"""

# example of loading the mnist dataset
from numpy import mean
from numpy import std
from sklearn import datasets
from sklearn.manifold import TSNE
from matplotlib import pyplot as plt
from sklearn.model_selection import KFold
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten, BatchNormalization
from tensorflow.keras.optimizers import SGD
from tensorflow.keras import regularizers
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
#from tensorflow.keras.datasets import cifar10,cifar100
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
import keras
import keras.backend as K
import numpy as np
def conc(*inp1):
  return layers.Concatenate()(inp1)

#!pip install tensorflow_addons
#import tensorflow_addons as tfa

def mpool(psize,strides=2):
  return MaxPooling2D(pool_size=psize,strides=strides,padding="valid")

def apool(psize,strides=None):
  if(strides is None):
    return AveragePooling2D(pool_size=psize,padding='same')
  else:
    return AveragePooling2D(pool_size=psize,strides=strides,padding="same")

def ln():
  return layers.LayerNormalization()

def bn():
  return layers.BatchNormalization()

from keras.callbacks import ModelCheckpoint
import tensorflow.keras.layers as layers

learning_rate = 0.0001
lr_drop=7
def lr_scheduler(epoch):
        return learning_rate * (0.5 ** (epoch // lr_drop))

reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)

def train(model,path,epochs=100):

  checkpoint1 = ModelCheckpoint(filepath='./'+path,save_format=tf,monitor='val_loss',
                            save_weights_only=True,
                             verbose=1,
                             save_best_only=True,
                             mode='min')
  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate,momentum=0.9,clipvalue=0.01,global_clipnorm=0.01),
    loss='categorical_crossentropy',
    metrics='categorical_accuracy')
  model.fit(trainXA, trainY, epochs=epochs, batch_size=200, validation_data=(testXA, testY), callbacks=[checkpoint1, reduce_lr], verbose=1)

def bnconv(inp,units,kernel_size):
  return BatchNormalization()(Conv2D(units,kernel_size,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005))(inp))

def dense(units,act='relu'):
  return layers.Dense(units,activation=act)

def conv(inp,units,kernel_size):
  return Conv2D(kernel_size,units,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005))(inp)

#L21 regularization layer
@tf.keras.utils.register_keras_serializable(package='Custom', name='l21')
class L21(regularizers.Regularizer):
    def __init__(self, mu):
        self.mu = mu

    def __call__(self, w):
        return self.mu*tf.math.reduce_sum(tf.sqrt(tf.reduce_sum(w**2,1)))

    def get_config(self):
        return {'mu': self.mu}

#L21 regularization layer for convolutions
@tf.keras.utils.register_keras_serializable(package='Custom', name='l21c')
class L21C(regularizers.Regularizer):
    def __init__(self, mu):
        self.mu = mu

    def __call__(self, w):
        return self.mu*tf.math.reduce_sum(tf.sqrt(tf.reduce_sum(w**2,[0,1,3])))

    def get_config(self):
        return {'mu': self.mu}

'''The Robust Margin Prototypical (RMPE) classifier layer, an alternative to the softmax layer that is robust to noise and adversarial attacks'''
class RMPE(keras.layers.Layer):
    def __init__(self, clusters=200,contrastive=True,activation='softmax',margin=True,**kwargs):
        super(RMPE, self).__init__(**kwargs)
        self.clusters = clusters;
        self.activation=activation; self.contrastive=contrastive; self.margin=margin;

    def build(self, input_shape):
        self.mu = self.add_weight(name='mu',
                                  shape=([input_shape[-1],self.clusters]),
                                  initializer='he_normal',
                                  trainable=True)
        self.inputshape = input_shape
        super(RMPE, self).build(input_shape)

    def call(self, inputs):
        if(self.margin):
            q = tf.reduce_min(tf.reduce_sum((self.mu[:,tf.newaxis,:]-self.mu[:,:,tf.newaxis])**2,0),-1) + 0.001*tf.reduce_sum(tf.sqrt(tf.reduce_sum(tf.square(self.mu),1)))
            self.add_loss(0.003*tf.reduce_sum(tf.where(q<0.7,q,0.0)))

        diff = tf.expand_dims(inputs,-1) - self.mu
        l2 = tf.abs(diff)
        l2 = tf.reduce_sum(l2,-2)

        if self.activation=='softmax':
            l = -l2
            res = tf.keras.activations.softmax(l)
        else:
            res = tf.keras.activations.sigmoid(-l2)
        return res

#The Regular CNN classifier with softmax activations
def VanillaSoftmaxModel():

  #Encoder layer
  inputs = layers.Input(shape=(28, 28, 1))
  inputsi = inputs[:,:,:,0:1]
  f = bn()(layers.Conv2D(kernel_size=3,activation='relu',filters=68,kernel_initializer='glorot_normal',kernel_regularizer=keras.regularizers.L2(0.0003),padding='SAME')(inputsi))
  l1 = mpool(2,strides=2)(f)#tf.reduce_mean(f,-2); #orientation = apool(4,strides=2)(tf.cast(orientation,dtype=tf.float32))
  l1 = bn()(layers.Conv2D(kernel_size=3,activation='relu',filters=200,kernel_initializer='glorot_normal',kernel_regularizer=keras.regularizers.L2(0.0003),padding='SAME')(l1)) #MultiScaleConv(filters=200,kernel_size=(3,3))
  l1 = mpool(2,strides=2)(l1)#tf.reduce_mean(f,-2);
  l1 = bn()(layers.Conv2D(kernel_size=3,activation='relu',filters=200,kernel_initializer='glorot_normal',kernel_regularizer=keras.regularizers.L2(0.0003),padding='SAME')(l1))
  l1 = mpool(2,strides=2)(l1)
  l1 = bn()(layers.Conv2D(kernel_size=3,activation='relu',filters=356,kernel_initializer='glorot_normal',kernel_regularizer=keras.regularizers.L2(0.0003),padding='SAME')(l1))
  l1 = mpool(2,strides=2)(l1)

  li = layers.Flatten()(l1)
  li = layers.Dense(640,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(0.0008),activity_regularizer=tf.keras.regularizers.L1(0.0006))(li)
  x = layers.Dense(10,activation='softmax')(li)
  return tf.keras.Model(inputs = inputs,outputs = x), tf.keras.Model(inputs = inputs,outputs = lr), tf.keras.Model(inputs = inputs,outputs = lr)

def RobustMarginPrototypeModel():
  #Encoder layer
  inputs = layers.Input(shape=(28, 28, 1))
  f = bn()(layers.Conv2D(kernel_size=3,activation='relu',filters=68,kernel_initializer='glorot_normal',kernel_regularizer=L21C(0.0003),padding='same')(inputs))
  l1 = mpool(2,strides=2)(f)#tf.reduce_mean(f,-2); #orientation = apool(4,strides=2)(tf.cast(orientation,dtype=tf.float32))
  l1 = bn()(layers.Conv2D(kernel_size=3,activation='relu',filters=200,kernel_initializer='glorot_normal',kernel_regularizer=L21C(0.0003),padding='same')(l1)) #MultiScaleConv(filters=200,kernel_size=(3,3))
  l1 = mpool(2,strides=2)(l1)#tf.reduce_mean(f,-2);
  l1 = bn()(layers.Conv2D(kernel_size=3,activation='relu',filters=200,kernel_initializer='glorot_normal',kernel_regularizer=L21C(0.0003),padding='same')(l1))
  l1 = mpool(2,strides=2)(l1)
  l1 = bn()(layers.Conv2D(kernel_size=3,activation='relu',filters=356,kernel_initializer='glorot_normal',kernel_regularizer=L21C(0.0003),padding='same')(l1))
  l1 = mpool(2,strides=2)(l1)

  li = layers.Flatten()(l1)
  li = layers.Dense(640,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(0.0008),activity_regularizer=tf.keras.regularizers.L1(0.0006))(li)

  x = RMPE(10,activation='softmax',contrastive=False,margin=True)(li)

  #Decoder Layer
  lr = layers.Dense(500,activation='relu',kernel_regularizer=tf.keras.regularizers.L1(0.0008))(x)
  lr = layers.UpSampling2D(size=2)(lr[:,tf.newaxis,tf.newaxis,:])
  lr = layers.UpSampling2D(size=2)(lr);

  lr = layers.Conv2DTranspose(kernel_size=3,activation='relu',filters=356,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.L1(0.0004),padding='same')(lr)
  lr = layers.UpSampling2D(size=2)(lr)
  lr = layers.Conv2DTranspose(kernel_size=3,activation='relu',filters=200,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.L1(0.0004),padding='same')(lr)
  lr = layers.UpSampling2D(size=2)(lr)
  lr = layers.Conv2D(kernel_size=3,activation='relu',filters=200,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.L1(0.0004),padding='valid')(lr)
  lr = layers.UpSampling2D(size=2)(lr)
  lr = layers.Conv2DTranspose(kernel_size=3,activation='relu',filters=64,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.L1(0.0004),padding='same')(lr)
  lr = layers.Dense(1,activation='sigmoid')(lr)

  return tf.keras.Model(inputs = inputs,outputs = x), tf.keras.Model(inputs = inputs,outputs = lr)